{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "372ff1cd",
   "metadata": {},
   "source": [
    "# CycleGAN — Bug-Fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60f1544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import itertools\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ---------------------------\n",
    "# Data  (normalize BOTH domains to [-1,1] for Tanh)   [SE-1]\n",
    "# ---------------------------\n",
    "norm = transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "\n",
    "tf_X = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    norm                                                    # FIX [SE-1]\n",
    "])\n",
    "tf_Y = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    norm                                                    # FIX [SE-1] (same normalization)\n",
    "])\n",
    "\n",
    "class UnpairedCIFAR(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        self.X = torchvision.datasets.CIFAR10('./data', train=train, download=True, transform=tf_X)\n",
    "        self.Y = torchvision.datasets.CIFAR10('./data', train=train, download=True, transform=tf_Y)\n",
    "    def __len__(self): return min(len(self.X), len(self.Y))\n",
    "    def __getitem__(self, i):\n",
    "        x,_ = self.X[i]\n",
    "        y,_ = self.Y[(i*7) % len(self.Y)]\n",
    "        return x, y\n",
    "\n",
    "ds = UnpairedCIFAR(train=True)\n",
    "loader = DataLoader(ds, batch_size=4, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Models  (InstanceNorm in G; logits/no Sigmoid in D)   [E-5][SE-2]\n",
    "# ---------------------------\n",
    "def c7s1(in_c, out_c):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, 7, 1, 3),\n",
    "        nn.InstanceNorm2d(out_c),                            # FIX [E-5]\n",
    "        nn.ReLU(True)\n",
    "    )\n",
    "\n",
    "def d_block(in_c, out_c):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, 3, 2, 1),\n",
    "        nn.InstanceNorm2d(out_c),                            # FIX [E-5]\n",
    "        nn.ReLU(True)\n",
    "    )\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(ch, ch, 3, 1, 1), nn.InstanceNorm2d(ch), nn.ReLU(True),\n",
    "            nn.Conv2d(ch, ch, 3, 1, 1), nn.InstanceNorm2d(ch)\n",
    "        )\n",
    "    def forward(self, x): return x + self.block(x)\n",
    "\n",
    "class ResGen(nn.Module):\n",
    "    def __init__(self, in_ch=3, ch=64, n_blocks=6):\n",
    "        super().__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            c7s1(in_ch, ch),\n",
    "            d_block(ch, ch*2),\n",
    "            d_block(ch*2, ch*4),\n",
    "        )\n",
    "        self.res = nn.Sequential(*[ResBlock(ch*4) for _ in range(n_blocks)])\n",
    "        self.up  = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ch*4, ch*2, 3, 2, 1, output_padding=1), nn.InstanceNorm2d(ch*2), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ch*2, ch,   3, 2, 1, output_padding=1), nn.InstanceNorm2d(ch),   nn.ReLU(True),\n",
    "            nn.Conv2d(ch, 3, 7, 1, 3),\n",
    "            nn.Tanh()                                            # use Tanh to match [-1,1] inputs     [SE-1]\n",
    "        )\n",
    "    def forward(self, x): return self.up(self.res(self.down(x)))\n",
    "\n",
    "class PatchD(nn.Module):\n",
    "    def __init__(self, in_ch=3, ch=64):\n",
    "        super().__init__()\n",
    "        # no BN/IN in D is common; we keep it simple and stable\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, ch,   4, 2, 1), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(ch,   ch*2,  4, 2, 1), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(ch*2, ch*4,  4, 2, 1), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(ch*4, 1,     1, 1, 0)  # logits, NO Sigmoid                            [SE-2]\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)  # (B,1,H',W')\n",
    "\n",
    "# Two generators and TWO discriminators                    [H-10]\n",
    "G_XY = ResGen().to(device)   # X → Y\n",
    "G_YX = ResGen().to(device)   # Y → X\n",
    "D_X  = PatchD().to(device)   # judges in domain X\n",
    "D_Y  = PatchD().to(device)   # judges in domain Y\n",
    "\n",
    "# ---------------------------\n",
    "# Losses & Optimizers\n",
    "# ---------------------------\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "l1  = nn.L1Loss()\n",
    "lambda_cyc = 10.0\n",
    "\n",
    "optG  = torch.optim.Adam(itertools.chain(G_XY.parameters(), G_YX.parameters()), lr=2e-4, betas=(0.5, 0.999))\n",
    "optDX = torch.optim.Adam(D_X.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "optDY = torch.optim.Adam(D_Y.parameters(), lr=2e-4, betas=(0.5, 0.999))      # separate optims         [E-7]\n",
    "\n",
    "# Proper cycle & identity losses                           [M-8][E-6]\n",
    "def cycle_consistency_loss(G_XY, G_YX, x, y, lam=10.0):\n",
    "    cyc_x = G_YX(G_XY(x))\n",
    "    cyc_y = G_XY(G_YX(y))\n",
    "    return lam * (F.l1_loss(cyc_x, x) + F.l1_loss(cyc_y, y))        # FIX [M-8]\n",
    "\n",
    "def identity_loss(G_XY, G_YX, x, y, lam=10.0):\n",
    "    id_x = G_YX(x)\n",
    "    id_y = G_XY(y)\n",
    "    return 0.5 * lam * (F.l1_loss(id_x, x) + F.l1_loss(id_y, y))    # FIX [E-6]\n",
    "\n",
    "# ---------------------------\n",
    "# Training (short sanity run)\n",
    "# ---------------------------\n",
    "G_XY.train(); G_YX.train(); D_X.train(); D_Y.train()\n",
    "for step, (x, y) in enumerate(loader):\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    # ---- Discriminator X ----\n",
    "    with torch.no_grad():                                             # or .detach()\n",
    "        fake_x = G_YX(y)                                             # Y→X\n",
    "    optDX.zero_grad()\n",
    "    logits_real_X = D_X(x)\n",
    "    logits_fake_X = D_X(fake_x)\n",
    "    lossDX = bce(logits_real_X, torch.ones_like(logits_real_X)) + bce(logits_fake_X, torch.zeros_like(logits_fake_X))  # targets match patch shape   [SE-3]\n",
    "            \n",
    "    lossDX.backward()\n",
    "    optDX.step()\n",
    "\n",
    "    # ---- Discriminator Y ----\n",
    "    with torch.no_grad():\n",
    "        fake_y = G_XY(x)                                             # X→Y\n",
    "    optDY.zero_grad()\n",
    "    logits_real_Y = D_Y(y)\n",
    "    logits_fake_Y = D_Y(fake_y)\n",
    "    lossDY = bce(logits_real_Y, torch.ones_like(logits_real_Y)) + \\\n",
    "            bce(logits_fake_Y, torch.zeros_like(logits_fake_Y))\n",
    "    lossDY.backward()\n",
    "    optDY.step()\n",
    "\n",
    "    # ---- Generators (adv + cycle + identity) ----\n",
    "    optG.zero_grad()\n",
    "    fake_x = G_YX(y)                                                 # recompute (no detach)    [M-9]\n",
    "    fake_y = G_XY(x)\n",
    "    adv_X = bce(D_X(fake_x), torch.ones_like(D_X(fake_x)))           # want fakes judged real\n",
    "    adv_Y = bce(D_Y(fake_y), torch.ones_like(D_Y(fake_y)))\n",
    "    loss_cyc = cycle_consistency_loss(G_XY, G_YX, x, y, lambda_cyc)   # FIX [M-8]\n",
    "    loss_id  = identity_loss(G_XY, G_YX, x, y, lambda_cyc)            # FIX [E-6]\n",
    "    lossG = adv_X + adv_Y + loss_cyc + loss_id\n",
    "    lossG.backward()\n",
    "    optG.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"step {step:04d} | DX={lossDX.item():.3f} DY={lossDY.item():.3f} G={lossG.item():.3f} (advX={adv_X.item():.3f}, advY={adv_Y.item():.3f}, cyc={loss_cyc.item():.3f}, id={loss_id.item():.3f})\")\n",
    "    if step == 400:  # small sanity run\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a8321",
   "metadata": {},
   "source": [
    "## what was fixed (mapping to bug tags)\n",
    "\n",
    "1. **\\[SE-1]** identical normalization for both domains to `(-1,1)` and `Tanh` generator output range.\n",
    "2. **\\[SE-2]** discriminators output **logits** (no Sigmoid) when using `BCEWithLogitsLoss`.\n",
    "3. **\\[SE-3]** patch targets now use `ones_like/zeros_like(logits)` to match spatial shape.\n",
    "4. **\\[SE-4]** fakes **detached** (or computed under `no_grad`) during D updates.\n",
    "5. **\\[E-5]** **InstanceNorm2d** in generators (and no BN in discriminators).\n",
    "6. **\\[E-6]** added **identity loss**: `0.5*λ*(|G_YX(x)-x|₁ + |G_XY(y)-y|₁)`.\n",
    "7. **\\[E-7]** separate discriminators **D\\_X** and **D\\_Y** with separate optimizers **optDX/optDY**.\n",
    "8. **\\[M-8]** correct **cycle consistency**: `λ*(|G_YX(G_XY(x))-x|₁ + |G_XY(G_YX(y))-y|₁)`.\n",
    "9. **\\[M-9]** recompute fakes for generator step (no detach) and manage `zero_grad()/step()` in the proper order.\n",
    "10. **\\[H-10]** replaced the single shared D with **two discriminators** and stepped the **correct** optimizer for each update.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
